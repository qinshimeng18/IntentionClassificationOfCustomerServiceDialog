{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim,jieba,os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w =  Word2Vec.load('./self_train_word2vec')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_embedding(w):\n",
    "    index = 0\n",
    "    word2id = {}\n",
    "    vec = []\n",
    "    for word in w.wv.vocab.keys():\n",
    "        word2id[word] = index\n",
    "        vec.append(w[word])\n",
    "        index +=1\n",
    "    #add two words\n",
    "    word2id['UNK'] = len(word2id) # 位置字符代替id\n",
    "    word2id['BLANK'] = len(word2id) # 临时填充\n",
    "    vec.append(np.random.normal(size=100, loc=0, scale=0.05))\n",
    "    vec.append(np.random.normal(size=100, loc=0, scale=0.05))\n",
    "    vec = np.array(vec, dtype=np.float32) #[[100]...]\n",
    "    vec_dict = {}\n",
    "    for i in range(vec.shape[0]):\n",
    "        vec_dict[i] = vec[i]\n",
    "    return vec,word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2524, 100) 2524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucas/anaconda/envs/py3/lib/python3.6/site-packages/ipykernel/__main__.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "vec,word2id = load_embedding(w)\n",
    "print(vec.shape,len(word2id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X  \n",
    "## 转换成句子id   [id,id,id,id....]-生成x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucas/anaconda/envs/py3/lib/python3.6/site-packages/ipykernel/__main__.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "# df = pd.read_csv('cleandemo8.csv')\n",
    "# df = pd.read_csv('data_labeled_2.csv')\n",
    "df =pd.read_csv('./df_cluster.csv')\n",
    "# df = pd.read_csv('./clustering_withKeyword.csv')\n",
    "# df = pd.read_csv('/Users/lucas/Desktop/workspace/IntentionClassify/clustering_withLSTM.csv')\n",
    "# df = pd.read_csv('/Users/lucas/Desktop/workspace/IntentionClassify/clustering_withIDF.csv')\n",
    "sens_vec = []\n",
    "black = np.array([0]*w['话费'].shape[0])\n",
    "data_word = []\n",
    "fix_lenght = 20\n",
    "c = 1\n",
    "for sen in df.request:\n",
    "    sen_vec = [word2id[word] if word in word2id else word2id['UNK'] for word in jieba.cut(sen)]\n",
    "    sen_vec = sen_vec[:fix_lenght]\n",
    "    while len(sen_vec) < fix_lenght:\n",
    "        sen_vec.append(word2id['BLANK'])   \n",
    "    data_word.append(sen_vec)\n",
    "    c+=1\n",
    "    if c>10000:\n",
    "        break\n",
    "data_word = np.array(data_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6132, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sessionid</th>\n",
       "      <th>request</th>\n",
       "      <th>response</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13410000258T16081911402229APP</td>\n",
       "      <td>实名制了怎么还不能打电话？</td>\n",
       "      <td>请问是本机吗？我帮您开机，请稍等哈</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13410000422T16082318212113APP</td>\n",
       "      <td>查询我现在的套餐</td>\n",
       "      <td>，您改了38元4G飞享套餐，9-13号生效哦，还在线吗，还有其他可以帮到您吗？</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       sessionid        request  \\\n",
       "0  13410000258T16081911402229APP  实名制了怎么还不能打电话？   \n",
       "1  13410000422T16082318212113APP       查询我现在的套餐   \n",
       "\n",
       "                                  response  cluster  \n",
       "0                        请问是本机吗？我帮您开机，请稍等哈        2  \n",
       "1  ，您改了38元4G飞享套餐，9-13号生效哦，还在线吗，还有其他可以帮到您吗？        3  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_csv('./data_labeled_2.csv')\n",
    "print(df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0. 0. 0. 0.] 2 7\n"
     ]
    }
   ],
   "source": [
    "name = set(df.cluster)\n",
    "label_dict = dict(zip(name,range(len(name))))\n",
    "label_ids = [label_dict[word] for word in df.cluster]\n",
    "y = np.eye(len(label_dict))[label_ids]\n",
    "print(y[0],label_ids[0],len(label_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers import Flatten,Activation\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cut = int(data_word.shape[0]*0.7)\n",
    "# tr_x = data_word[:cut]\n",
    "# tr_y = y[:cut]\n",
    "# ts_x = data_word[cut:]\n",
    "# ts_y = y[cut:]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "tr_x, ts_x, tr_y, ts_y = train_test_split(data_word, y, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "model = Sequential()\n",
    "vocab_size = len(vec) \n",
    "model.add(Embedding(vocab_size, 100, weights=[vec], input_length=20, trainable=False))\n",
    "model.add(Bidirectional(LSTM(25,dropout=0.4,return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(24, activation='relu')))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(14, activation='sigmoid'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 20, 100)           252400    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 20, 25)            12600     \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 20, 24)            624       \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 480)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 14)                6734      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 7)                 105       \n",
      "=================================================================\n",
      "Total params: 272,463\n",
      "Trainable params: 20,063\n",
      "Non-trainable params: 252,400\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\"\"\"\n",
    "All that the Embedding layer does is to map the integer inputs to the vectors found at the corresponding index \n",
    "in the embedding matrix, i.e. the sequence [1, 2] would be converted to [embeddings[1], embeddings[2]]. \n",
    "This means that the output of the Embedding layer will be a 3D tensor of shape \n",
    "(samples, sequence_length, embedding_dim)\n",
    "\"\"\"\n",
    "vocab_size = len(vec) # 最大的词数，与词向量实际数目有关\n",
    "model.add(Embedding(vocab_size, 100, weights=[vec], input_length=20, trainable=False))\n",
    "# model.add(Bidirectional(LSTM(25,dropout=0.4,return_sequences=True),merge_mode='concat'))\n",
    "model.add(LSTM(25,dropout=0.4,return_sequences=True))\n",
    "# model.add(Activation('softmax')) #this guy here\n",
    "model.add(TimeDistributed(Dense(24, activation='relu')))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(14, activation='sigmoid'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# model.add((LSTM(50,dropout=0.3,kernel_initializer='random_normal',input_shape=(100,10))))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# print(model.summary())\n",
    "# fit the model\n",
    "# data_words = data_word[:100]\n",
    "# ys = np.reshape(y[:100],(y[:100].shape[0],1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2024, 20)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4108 samples, validate on 2024 samples\n",
      "Epoch 1/20\n",
      "4108/4108 [==============================] - 4s 992us/step - loss: 1.8193 - categorical_accuracy: 0.2821 - val_loss: 1.7556 - val_categorical_accuracy: 0.3192\n",
      "Epoch 2/20\n",
      "4108/4108 [==============================] - 2s 592us/step - loss: 1.7462 - categorical_accuracy: 0.3186 - val_loss: 1.7105 - val_categorical_accuracy: 0.3552\n",
      "Epoch 3/20\n",
      "4108/4108 [==============================] - 2s 533us/step - loss: 1.7145 - categorical_accuracy: 0.3354 - val_loss: 1.6810 - val_categorical_accuracy: 0.3646\n",
      "Epoch 4/20\n",
      "4108/4108 [==============================] - 2s 539us/step - loss: 1.6937 - categorical_accuracy: 0.3444 - val_loss: 1.6695 - val_categorical_accuracy: 0.3622\n",
      "Epoch 5/20\n",
      "4108/4108 [==============================] - 2s 504us/step - loss: 1.6740 - categorical_accuracy: 0.3525 - val_loss: 1.6599 - val_categorical_accuracy: 0.3617\n",
      "Epoch 6/20\n",
      "4108/4108 [==============================] - 2s 480us/step - loss: 1.6678 - categorical_accuracy: 0.3576 - val_loss: 1.6557 - val_categorical_accuracy: 0.3646\n",
      "Epoch 7/20\n",
      "4108/4108 [==============================] - 2s 518us/step - loss: 1.6622 - categorical_accuracy: 0.3617 - val_loss: 1.6440 - val_categorical_accuracy: 0.3661\n",
      "Epoch 8/20\n",
      "4108/4108 [==============================] - 2s 509us/step - loss: 1.6400 - categorical_accuracy: 0.3722 - val_loss: 1.6438 - val_categorical_accuracy: 0.3622\n",
      "Epoch 9/20\n",
      "4108/4108 [==============================] - 2s 501us/step - loss: 1.6346 - categorical_accuracy: 0.3717 - val_loss: 1.6362 - val_categorical_accuracy: 0.3641\n",
      "Epoch 10/20\n",
      "4108/4108 [==============================] - 2s 519us/step - loss: 1.6210 - categorical_accuracy: 0.3737 - val_loss: 1.6333 - val_categorical_accuracy: 0.3686\n",
      "Epoch 11/20\n",
      "4108/4108 [==============================] - 2s 533us/step - loss: 1.6173 - categorical_accuracy: 0.3807 - val_loss: 1.6324 - val_categorical_accuracy: 0.3686\n",
      "Epoch 12/20\n",
      "4108/4108 [==============================] - 2s 523us/step - loss: 1.6084 - categorical_accuracy: 0.3744 - val_loss: 1.6342 - val_categorical_accuracy: 0.3701\n",
      "Epoch 13/20\n",
      "4108/4108 [==============================] - 2s 493us/step - loss: 1.6034 - categorical_accuracy: 0.3778 - val_loss: 1.6320 - val_categorical_accuracy: 0.3666\n",
      "Epoch 14/20\n",
      "4108/4108 [==============================] - 2s 522us/step - loss: 1.5926 - categorical_accuracy: 0.3917 - val_loss: 1.6339 - val_categorical_accuracy: 0.3691\n",
      "Epoch 15/20\n",
      "4108/4108 [==============================] - 2s 504us/step - loss: 1.5903 - categorical_accuracy: 0.3829 - val_loss: 1.6271 - val_categorical_accuracy: 0.3785\n",
      "Epoch 16/20\n",
      "4108/4108 [==============================] - 2s 505us/step - loss: 1.5758 - categorical_accuracy: 0.3968 - val_loss: 1.6260 - val_categorical_accuracy: 0.3735\n",
      "Epoch 17/20\n",
      "4108/4108 [==============================] - 2s 490us/step - loss: 1.5646 - categorical_accuracy: 0.3975 - val_loss: 1.6335 - val_categorical_accuracy: 0.3804\n",
      "Epoch 18/20\n",
      "4108/4108 [==============================] - 2s 537us/step - loss: 1.5618 - categorical_accuracy: 0.4075 - val_loss: 1.6237 - val_categorical_accuracy: 0.3765\n",
      "Epoch 19/20\n",
      "4108/4108 [==============================] - 2s 538us/step - loss: 1.5591 - categorical_accuracy: 0.4000 - val_loss: 1.6261 - val_categorical_accuracy: 0.3790\n",
      "Epoch 20/20\n",
      "4108/4108 [==============================] - 2s 531us/step - loss: 1.5478 - categorical_accuracy: 0.4085 - val_loss: 1.6279 - val_categorical_accuracy: 0.3785\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(tr_x, tr_y, epochs=20, verbose=1,batch_size=50,validation_data=(ts_x,ts_y))\n",
    "# evaluate the model\n",
    "\n",
    "# print(history.history['categorical_accuracy'])\n",
    "# loss, accuracy = model.evaluate(ts_x, ts_y, verbose=0)\n",
    "# print('Accuracy: %f' % (accuracy*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6]), array([192, 260, 325, 306, 191, 646, 104]))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model.predict_classes(ts_x)\n",
    "ts_y_classes = [list(x).index(1) for x in ts_y]\n",
    "np.unique(ts_y_classes,return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3784584980237154\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.40      0.09      0.15       192\n",
      "          1       0.37      0.27      0.31       260\n",
      "          2       0.22      0.11      0.15       325\n",
      "          3       0.33      0.13      0.18       306\n",
      "          4       0.39      0.45      0.42       191\n",
      "          5       0.40      0.79      0.53       646\n",
      "          6       0.50      0.05      0.09       104\n",
      "\n",
      "avg / total       0.36      0.38      0.32      2024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score,accuracy_score,classification_report\n",
    "print(accuracy_score(ts_y_classes,res))\n",
    "print(classification_report(ts_y_classes,res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bseline：  acy: 0.4408 - val_loss: 1.6413 - val_categorical_accuracy: 0.3715\n",
    "加权：acy: 0.6254 - val_loss: 1.1968 - val_categorical_accuracy: 0.5860 \n",
    "复杂模型：acy: 0.6741 - val_loss: 1.2125 - val_categorical_accuracy: 0.6472\n",
    "tf-idf：acy: 0.7047 - val_loss: 0.7166 - val_categorical_accuracy: 0.7085\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_categorical_accuracy', 'loss', 'categorical_accuracy'])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6]),\n",
       " array([ 561,  757, 1039,  920,  608, 1936,  311]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df.cluster, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6]),\n",
       " array([  78,  183,  138,  186,  275, 1157,    7]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(res, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "752 0.3715415019762846\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(ts_y_classes)):\n",
    "    if ts_y_classes[i]==res[i]:\n",
    "        count+=1\n",
    "print(count,count/len(ts_y_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "model = Sequential()\n",
    "\n",
    "vocab_size = len(vec) # 最大的词数，与词向量实际数目有关\n",
    "model.add(Embedding(vocab_size, 100, weights=[vec], input_length=20, trainable=False))\n",
    "# model.add(LSTM(5,kernel_initializer='random_normal',input_shape=(20,100),return_sequences=True))\n",
    "model.add(LSTM(5,dropout=0.3,return_sequences=True))\n",
    "# model.add(Flatten())\n",
    "# model.add(TimeDistributed(Dense(5)))\n",
    "# model.add(TimeDistributed(Dense(2, activation='sigmoid')))\n",
    "model.add(Dense(15, activation='softmax'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# model.add((LSTM(50,dropout=0.3,kernel_initializer='random_normal',input_shape=(100,10))))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# print(model.summary())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # fit the model\n",
    "# data_words = data_word[:100]\n",
    "# ys = np.reshape(y[:100],(y[:100].shape[0],1,1))\n",
    "# model.fit(data_words, ys, epochs=20, verbose=0,batch_size=10)\n",
    "# # evaluate the model\n",
    "# loss, accuracy = model.evaluate(data_words, ys, verbose=0)\n",
    "# print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 7)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(sens_vec,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'one_hot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-904687e8a1ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# integer encode the documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mencoded_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-88-904687e8a1ec>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# integer encode the documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mencoded_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'one_hot' is not defined"
     ]
    }
   ],
   "source": [
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "# define class labels\n",
    "labels = np.array([1,1,1,1,1,0,0,0,0,0])\n",
    "# integer encode the documents\n",
    "vocab_size = 50\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "# define example\n",
    "data = [1, 3, 2, 0, 3, 2, 2, 1, 0, 1]\n",
    "data = np.array(data)\n",
    "print(data)\n",
    "# one hot encode\n",
    "encoded = to_categorical(data)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "# define documents\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "# define class labels\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "print(encoded_docs)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)\n",
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('../glove_data/glove.6B/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "\tvalues = line.split()\n",
    "\tword = values[0]\n",
    "\tcoefs = asarray(values[1:], dtype='float32')\n",
    "\tembeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "\tembedding_vector = embeddings_index.get(word)\n",
    "\tif embedding_vector is not None:\n",
    "\t\tembedding_matrix[i] = embedding_vector\n",
    "# define model\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "294px",
    "left": "1003.09px",
    "right": "20px",
    "top": "8.99999px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
